{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiagu \n",
    "import jieba \n",
    "import pkuseg \n",
    "import genius \n",
    "import jieba.posseg as jieb_seg\n",
    "import pandas as pd \n",
    "from sklearn.metrics.pairwise import euclidean_distances as distance\n",
    "\n",
    "import json\n",
    "from tencentcloud.common import credential\n",
    "from tencentcloud.common.profile.client_profile import ClientProfile\n",
    "from tencentcloud.common.profile.http_profile import HttpProfile\n",
    "from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException\n",
    "from tencentcloud.nlp.v20190408 import nlp_client, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_obj:\n",
    "    \n",
    "    def __init__(self, word, *seg_results):\n",
    "        self.word = word \n",
    "        self.is_n = any(item=='n' for item in seg_results)\n",
    "        self.is_cn = all(item != 'eng' for item in seg_results) and all((not char.isdigit()) and char.isalpha() for char in word)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.word \n",
    "    \n",
    "    __str__ = __repr__ \n",
    "\n",
    "\n",
    "class vector_obj:\n",
    "    \n",
    "    def __init__(self, id_, title, vector):\n",
    "        self.id = id_ \n",
    "        self.title = title \n",
    "        self.vector = vector \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.id} {self.title}'\n",
    "    \n",
    "    __str__ = __repr__ \n",
    "\n",
    "\n",
    "def flatten_list(input_list):\n",
    "    return input_list[0] + flatten_list(input_list[1:]) if len(input_list) > 1 else input_list[0]\n",
    "\n",
    "\n",
    "def get_tencent_client():\n",
    "    with open('api_keys.txt', 'r') as filein: \n",
    "        api_id = filein.readline().partition(':')[1]\n",
    "        api_secret = filein.readline().partition(':')[1]\n",
    "    cred = credential.Credential(api_id, api_secret) \n",
    "    httpProfile = HttpProfile()\n",
    "    httpProfile.endpoint = \"nlp.tencentcloudapi.com\"\n",
    "\n",
    "    clientProfile = ClientProfile()\n",
    "    clientProfile.httpProfile = httpProfile\n",
    "    client = nlp_client.NlpClient(cred, \"ap-guangzhou\", clientProfile) \n",
    "    return client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv('items.csv')\n",
    "item_names = items.sample(n=100)['Used_title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case1 = '三星note10+256G手机'\n",
    "test_case2 = '只用过不到三天的switch lite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_tencent_client()\n",
    "word_request = models.WordEmbeddingRequest() \n",
    "sentence_request = models.SentenceEmbeddingRequest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result\n",
    "except:\n",
    "    result = [] \n",
    "\n",
    "total_rows = items.shape[0]\n",
    "for series in items.iterrows(): \n",
    "    current_index, series = series \n",
    "    id_, title, *_ = series \n",
    "    print(f'\\r{current_index:>{total_rows}}/{total_rows}', end='', flush=True)\n",
    "    if current_index < 9354: continue \n",
    "    \n",
    "    sentence_request.from_json_string(json.dumps({'Text': title}))\n",
    "    response = client.SentenceEmbedding(sentence_request)\n",
    "    vector = json.loads(response.to_json_string())['Vector']\n",
    "    result.append(vector_obj(id_, title, vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count \n",
    "id_counter = count(1)\n",
    "\n",
    "all_words = {} # word -> id\n",
    "title_to_word = []\n",
    "pku_seg = pkuseg.pkuseg(postag=True)\n",
    "previous_step = -1 \n",
    "\n",
    "while previous_step < total_rows:\n",
    "    try:\n",
    "        for series in items.iterrows(): \n",
    "            current_index, series = series \n",
    "            id_, title, *_ = series \n",
    "            if current_index < previous_step:\n",
    "                continue \n",
    "\n",
    "            print(f'\\r{current_index:>{total_rows}}/{total_rows}', end='', flush=True)\n",
    "            temp_result = [] \n",
    "            search_list = jieba.lcut_for_search(title) #+ jiagu.seg(item_names[index])\n",
    "            search_list = list(filter(lambda item: item != ' ', search_list))\n",
    "            for word in search_list:\n",
    "                jieba_pos = jieb_seg.lcut(word)\n",
    "                jiagu_pos = jiagu.pos([word])\n",
    "                pku_pos = pku_seg.cut(word)\n",
    "            #    print(jieba_pos, jiagu_pos, word)\n",
    "                temp_result.append(word_obj(word, jieba_pos[0].flag, jiagu_pos[0], pku_pos[0][1]))\n",
    "\n",
    "            record.append(result)\n",
    "            result_cleaned = list(filter(lambda item: item.is_n and item.is_cn, temp_result))\n",
    "            title_to_word.append([title, []])\n",
    "\n",
    "            for word in result_cleaned: \n",
    "                word_id = next(id_counter)\n",
    "                params = {'Text': word}\n",
    "                word_request.from_json_string(json.dumps(params))\n",
    "                current_response = client.WordEmbedding(word_request)\n",
    "                vector = json.loads(response.to_json_string())['Vector']\n",
    "                result.append(vector_obj(word_id, word, vector))\n",
    "                title_to_word[-1][-1].append(word_id)\n",
    "    except:\n",
    "        previous_step = current_index \n",
    "        continue \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_title(query):\n",
    "    global client, sentence_request, result\n",
    "    params = {'Text': query}\n",
    "    sentence_request.from_json_string(json.dumps(params))\n",
    "    response = client.SentenceEmbedding(sentence_request)\n",
    "    vector = json.loads(response.to_json_string())['Vector']\n",
    "    distance_map = []\n",
    "    for vector_ins in result: \n",
    "        euclidean = distance([vector, vector_ins.vector])[0].sum() \n",
    "        distance_map.append([euclidean, vector_ins])\n",
    "    distance_map.sort(key=lambda item: item[0])\n",
    "    return distance_map[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'Text': '搅拌器'}\n",
    "word_request.from_json_string(json.dumps(params))\n",
    "response1 = client.WordEmbedding(word_request)\n",
    "vector1 = json.loads(response1.to_json_string())['Vector'].copy()\n",
    "\n",
    "params = {'Text': '显示器'}\n",
    "word_request.from_json_string(json.dumps(params))\n",
    "response2 = client.WordEmbedding(word_request)\n",
    "vector2 = json.loads(response2.to_json_string())['Vector'].copy()\n",
    "\n",
    "distance([vector1, vector2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title('屏幕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items[items['Used_title'].str.contains('有线电', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
